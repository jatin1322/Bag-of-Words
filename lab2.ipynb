{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf \n",
        "#Loading dataset using tensorflow\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#Importing Some basic libraries \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import cv2\n",
        "import skimage.io as io\n",
        "import joblib\n",
        "import pylab as pl\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.feature import hog\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score \n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "class My_Kmeancluster:\n",
        "    def __init__(self, M, number,number_of_iterations):\n",
        "        self.K = number \n",
        "        self.number_of_iterations = number_of_iterations \n",
        "        self.num_examples, self.num_fea = M.shape \n",
        "        self.pfig = True \n",
        "        \n",
        "    def init_rancen(self, M): \n",
        "        centro = np.zeros((self.K, self.num_fea))  \n",
        "        for k in range(self.K): \n",
        "            centroid = M[np.random.choice(range(self.num_examples))] \n",
        "            centro[k] = centroid\n",
        "        return centro \n",
        "    \n",
        "    def cre_clus(self, M, centro):\n",
        "        clusters = [[] for _ in range(self.K)]\n",
        "        for point_idx, point in enumerate(M):\n",
        "            clcen = np.argmin(\n",
        "                np.sqrt(np.sum((point-centro)**2, axis=1))\n",
        "            )\n",
        "            clusters[clcen].append(point_idx)\n",
        "        return clusters \n",
        "    \n",
        "    def cal_ne_cen(self, clusters, M):\n",
        "        centro = np.zeros((self.K, self.num_fea)) \n",
        "        for idx, cluster in enumerate(clusters):\n",
        "            if len(cluster)!=0:\n",
        "              new_centroid = np.mean(M[cluster], axis=0) \n",
        "              centro[idx] = new_centroid\n",
        "        return centro\n",
        "    \n",
        "    def pre_clus(self, clusters, M):\n",
        "        y_pred = np.zeros(self.num_examples)\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            for sample_idx in cluster:\n",
        "                y_pred[sample_idx] = cluster_idx\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, M):\n",
        "        centro = self.init_rancen(M) \n",
        "        for _ in range(self.number_of_iterations):\n",
        "            clusters = self.cre_clus(M, centro) \n",
        "            previous_centroids = centro\n",
        "            centro = self.cal_ne_cen(clusters, M) \n",
        "            diff = centro - previous_centroids \n",
        "            if not diff.any():\n",
        "                break\n",
        "        y_pred = self.pre_clus(clusters, M) \n",
        "        return centro\n",
        "\n",
        "\n",
        "Array_for_pitures = []\n",
        "Classes_for_pitures=[]\n",
        "\n",
        "def CreateDictionary():\n",
        "    read_from_csv = pd.read_csv('dataset/fashion-mnist_train.csv')\n",
        "    global Array_for_pitures\n",
        "    global Classes_for_pitures\n",
        "    jkl = read_from_csv['label']\n",
        "    for x in jkl:\n",
        "        Classes_for_pitures.append(x)\n",
        "    d = read_from_csv.drop(\"label\",axis=1)\n",
        "    for i in range(0,d.shape[0]):\n",
        "        grid_data = d.iloc[i].to_numpy().reshape(28,28)\n",
        "        Array_for_pitures.append(grid_data.astype(np.uint8))\n",
        "    print (\"Dictionary created\")\n",
        "    return\n",
        "    \n",
        "    \n",
        "def computeHistogram():\n",
        "    global Array_for_pitures,Classes_for_pitures\n",
        "    key =[]\n",
        "    des = []\n",
        "    sift_descrip = cv2.xfeatures2d.SIFT_create(100)\n",
        "    image_classes2=[]\n",
        "    for i in range(len(Array_for_pitures)):\n",
        "        keypoints, descriptor = sift_descrip.detectAndCompute(Array_for_pitures[i], None)\n",
        "        if descriptor is None:\n",
        "            continue\n",
        "        image_classes2.append(Classes_for_pitures[i])\n",
        "        key.append(keypoints)\n",
        "        des.append(descriptor)\n",
        "\n",
        "\n",
        "    descriptors=np.vstack(des)\n",
        "    descriptors_float=descriptors.astype(float)\n",
        "    k = 225  #k means with k clusters\n",
        "    mean_k = My_Kmeancluster(descriptors_float, k, 30)\n",
        "    voc = mean_k.fit(descriptors_float) \n",
        "    \n",
        "    im_features = np.zeros((len(image_classes2), k), \"float32\")\n",
        "    for i in range(len(image_classes2)):\n",
        "        words_arr, distance_Arr = vq(des[i],voc)\n",
        "        for w in words_arr:\n",
        "            im_features[i][w] += 1\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    stdSlr = StandardScaler().fit(im_features)\n",
        "    im_features = stdSlr.transform(im_features)\n",
        "    \n",
        "    from sklearn.svm import LinearSVC\n",
        "    clf = LinearSVC(max_iter=1000) \n",
        "    clf.fit(im_features, np.array(image_classes2))\n",
        "    print (\"Histogram  Computed\")\n",
        "    training_names=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "    joblib.dump((clf, training_names, stdSlr, k, voc), \"bovw.pkl\", compress=3) \n",
        "\n",
        "\n",
        "def matchHistogram():\n",
        "    print (\"Histogram  Matching Going on\")\n",
        "    clf, classes_names, stdSlr, k, voc = joblib.load(\"bovw.pkl\")\n",
        "    reading = pd.read_csv('dataset/fashion-mnist_test.csv')\n",
        "    Classes_for_pitures=[]\n",
        "    image_label = reading['label']\n",
        "    for x in image_label:\n",
        "        Classes_for_pitures.append(x)\n",
        "    d = reading.drop(\"label\",axis=1)\n",
        "    Array_for_pitures = []    \n",
        "    for i in range(0,d.shape[0]):\n",
        "        grid_data = d.iloc[i].to_numpy().reshape(28,28)\n",
        "        Array_for_pitures.append(grid_data.astype(np.uint8))\n",
        "        \n",
        "    key =[]\n",
        "    des = []\n",
        "    sift_descrip = cv2.xfeatures2d.SIFT_create(100)\n",
        "    image_classes2=[]\n",
        "    for i in range(len(Array_for_pitures)):\n",
        "        keypoints, descriptor = sift_descrip.detectAndCompute(Array_for_pitures[i], None)\n",
        "        if descriptor is None:\n",
        "            continue\n",
        "        image_classes2.append(Classes_for_pitures[i])\n",
        "        key.append(keypoints)\n",
        "        des.append(descriptor)\n",
        "    \n",
        "    \n",
        "    im_features = np.zeros((len(image_classes2), k), \"float32\")\n",
        "    for i in range(len(image_classes2)):\n",
        "        words_arr, distance_Arr = vq(des[i],voc)\n",
        "        for w in words_arr:\n",
        "            im_features[i][w] += 1\n",
        "            \n",
        "    im_features = stdSlr.transform(im_features)\n",
        "    \n",
        "    correct_class =  [classes_names[i] for i in image_classes2] \n",
        "    predict_class =  [classes_names[i] for i in clf.predict(im_features)]\n",
        "    \n",
        "    count=0\n",
        "    tot_count=0\n",
        "    for i in range(len(correct_class)):\n",
        "        tot_count+=1\n",
        "        if(correct_class[i]==predict_class[i]):\n",
        "            count+=1\n",
        "    \n",
        "    print (\"Histograms  Matched\")\n",
        "    print (\"True_class =\"  + str(correct_class))\n",
        "    print (\"Prediction =\"  + str(predict_class))\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\"Total Number of Hits Are:\",count)\n",
        "    accuracy = accuracy_score(correct_class, predict_class)\n",
        "    print (\"Your overall classification accuracy is = \", accuracy)\n",
        "    print (\"Precison is\" , precision_score(correct_class, predict_class,average='micro'))\n",
        "    print (\"Recall is\" , recall_score(correct_class, predict_class ,average='micro'))\n",
        "    cm = confusion_matrix(correct_class, predict_class)\n",
        "    print (cm)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    print(\"Accuracy Of each class is:\")\n",
        "    cm.diagonal()\n",
        "    pl.matshow(cm)\n",
        "    pl.title('Confusion matrix')\n",
        "    pl.colorbar()\n",
        "    pl.show()\n",
        "    \n",
        "\n",
        "CreateDictionary()\n",
        "computeHistogram()\n",
        "matchHistogram()\n"
      ],
      "metadata": {
        "id": "imXlbFpNx20b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}